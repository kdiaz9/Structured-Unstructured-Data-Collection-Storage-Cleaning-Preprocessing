# Structured and Unstructured DataCollection, Storage, Cleaning and Preprocessing

Whether data scientists work on structured or unstructured data, they often spend a significant amount of their time collecting, storing, cleaning and preprocessing. This workshop introduces best practices, techniques, and tools for effectively conducting these tasks. Topics covered include:
- collecting and extracting data from website using Scrapy, BeautifulSoup and other relevant Python packages;
- preprocessing data formats like JSON, CSV, HTML/XML to repair corrupt files, extract and filter relevant information, or de-duplicate records;
- data cleaning and manipulation using packages Pandas, NumPy and other relevant Python packages
- data visualization and visual inspection with Python
- practices and techniques to handle missing values in structured data


## Section 1:
Collecting and extracting data from website using Scrapy, BeautifulSoup and other relevant Python packages;
Hands-on: build a web crawler to harvest data from websites

Friday, January 19th, 4:30 – 7:30pm

## Section 2:
Preprocessing data formats like JSON, CSV, HTML/XML to repair corrupt files, extract and filter relevant information, or de-duplicate records; Hands-on: repairing several text files so that they comply the standard formats, extract data points using text processing tools (prior to loading them into Pandas, etc.)

Saturday, January 20th, 9am-12pm

## Session 3:
Data cleaning and manipulation using packages Pandas, NumPy and other relevant Python packages, data visualization and visual inspection with Python; Hands-on: finding outliers, invalid values, coverage, calculate distributions of values and correct by scaling and normalization, visually inspect data sets.

Friday, January 26th, 4:30 – 7:30pm

## Session 4:
Practices and techniques to handle missing values in structured data;
Hands-on: impute values for data set with missing data, using nearest neighbors, probability distribution, etc.

Saturday, January 27th, 9am-12pm
